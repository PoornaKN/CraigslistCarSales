{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pylab import *\n",
    "import sys\n",
    "import numpy as np\n",
    "import skimage.color\n",
    "import skimage.io\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81406dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= nltk.stem.WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r\"C:\\Users\\91702\\Desktop\\Purdue\\AUD\\Silverado.xlsx\")\n",
    "shape(data)\n",
    "Description = data[\"description\"]\n",
    "shape(Description)\n",
    "Collection_tokenized= []\n",
    "\n",
    "for i in Description:\n",
    "    Tokenize = nltk.word_tokenize(i)\n",
    "    Collection_tokenized.append(Tokenize)\n",
    "shape(Collection_tokenized)\n",
    "Lower_List = []\n",
    "for i in Collection_tokenized:\n",
    "    Temp_list = []\n",
    "    for j in i:\n",
    "        Temp_list.append(j.lower())\n",
    "    Lower_List.append(Temp_list)\n",
    "Lemmatized_list = []\n",
    "for j in Lower_List:\n",
    "    Lemmatized_list_temp = [lemmatizer.lemmatize(i) for i in j if i.isalpha()]\n",
    "    Lemmatized_list.append(Lemmatized_list_temp)\n",
    "Stopwords_removed = []\n",
    "for i in Lemmatized_list :\n",
    "    Temp_list = [j for j in i if not j in stopwords.words('english')]\n",
    "    Stopwords_removed.append(Temp_list)\n",
    "shape(Stopwords_removed)\n",
    "Td_idf_pre = []\n",
    "for i in Stopwords_removed:\n",
    "    Temp_sentence = \"\"\n",
    "    for j in i:\n",
    "         Temp_sentence = Temp_sentence + \" \" + j\n",
    "    Td_idf_pre.append(Temp_sentence)\n",
    "shape(Td_idf_pre)\n",
    "Vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = Vectorizer.fit_transform(Td_idf_pre)\n",
    "true_k = 4\n",
    "model1 = KMeans(n_clusters=true_k)\n",
    "model1.fit(X)\n",
    "model1.cluster_centers_\n",
    "model1.fit_transform(X)\n",
    "model1clusters = model1.labels_.tolist()\n",
    "print(model1clusters)\n",
    "order_centroids = model1.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = Vectorizer.get_feature_names()\n",
    "for ind in order_centroids[1, :20]:\n",
    "    print(terms[ind]) \n",
    "for ind in order_centroids[0, :20]:\n",
    "    print(terms[ind])\n",
    "shape(model1clusters)\n",
    "data[\"clusters\"] = model1clusters\n",
    "data[\"New_desc\"] = Td_idf_pre\n",
    "data.to_csv(\"Vehicle new.csv\")\n",
    "data\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('vehicles.csv')\n",
    "\n",
    "\n",
    "df1 = df[['id', 'url', 'price', 'year', 'manufacturer',\n",
    "       'model', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status',\n",
    "       'transmission', 'type', 'paint_color', 'description', 'state']]\n",
    "\n",
    "df2 = df[['id', 'url', 'price', 'description','model','manufacturer']]\n",
    "\n",
    "\n",
    "df2.dropna(inplace=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df2[df2['model']== 'f-150'].sort_values(by=['price'])['price'].plot(kind='hist',bins = 40)\n",
    "\n",
    "df2[df2['model']== 'f-150'].sort_values(by=['price'])[['price','description']]\n",
    "\n",
    "model = df2[df2['model']== 'f-150']\n",
    "\n",
    "model.drop_duplicates(subset = 'description',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#model.to_csv('model_original.csv')\n",
    "\n",
    "new = pd.read_csv('model_original.csv')\n",
    "\n",
    "df3 = new\n",
    "\n",
    "# FUNCTIONS\n",
    "\n",
    "#STEP 1 - Tokenized each review in the collection\n",
    "def TOKEN(x):\n",
    "    z = nltk.word_tokenize(x)\n",
    "    return z\n",
    " \n",
    "#STEP 2 - lemmatized all the words for each review\n",
    "lemmatizer= nltk.stem.WordNetLemmatizer()\n",
    "def LEMMA(x):\n",
    "    lemmatized_token_d2 = [lemmatizer.lemmatize(token) for token in x if token.isalpha()]\n",
    "    return lemmatized_token_d2\n",
    "\n",
    "#STEP 2 lowered all the words\n",
    "def LOWER(s):\n",
    "    a = [x.lower() for x in s]\n",
    "        \n",
    "    return a\n",
    "#STEP 3 - removed Stop Words\n",
    "def STOP(x):\n",
    "    stop_words_removed= [token for token in x if not token in \n",
    "                     stopwords.words('english') if token.isalpha()]\n",
    "    return stop_words_removed\n",
    "\n",
    "def JOIN(x):\n",
    "    q = ' '.join(str(e) for e in x) \n",
    "    return q\n",
    "\n",
    "\n",
    "df3.drop_duplicates(subset='description',inplace = True)\n",
    "\n",
    "\n",
    "df3['tokenised'] = df3['description'].apply(TOKEN)\n",
    "df3['Lametise'] = df3['tokenised'].apply(LEMMA)\n",
    "df3['Lower'] = df3['Lametise'].apply(LOWER)\n",
    "df3['Stop'] = df3['Lower'].apply(STOP)\n",
    "df3['Stop_new'] = df3['Stop'].apply(JOIN)\n",
    "\n",
    "def POS(x):\n",
    "    q = nltk.pos_tag(x) \n",
    "    return q\n",
    "\n",
    "\n",
    "list1 = list(df3['Stop'])\n",
    "\n",
    "Abbreviation = [\n",
    "'NN',\n",
    "'NNS',\n",
    "'NNP',\n",
    "'NNPS',\n",
    "'PDT',\n",
    "'POS',\n",
    "'RB',\n",
    "'RBR',\n",
    "'RBS',\n",
    "'VB',\n",
    "'VBG',\n",
    "'VBD',\n",
    "'VBN',\n",
    "'VBP',\n",
    "'VBZ',\n",
    "'WRB']\n",
    "\n",
    "\n",
    "fin_list = []\n",
    "for y in range (len(list1)):\n",
    "    list2 =[]\n",
    "    for x in range(len(list1[y])):\n",
    "        if POS(list1[y])[x][1] in Abbreviation :\n",
    "            list2.append(POS(list1[y])[x][0])\n",
    "        \n",
    "    fin_list.append(list2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "df3[['id','price', 'Spam', 'description']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df3['Stop_new']  \n",
    "y = df3['Spam']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) \n",
    "X_train_tfidf.shape\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  \n",
    "\n",
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))\n",
    "\n",
    "stopwords_rem = list(df3['Stop_new'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer( min_df=5)\n",
    "vectorizer.fit(stopwords_rem)\n",
    "cv = vectorizer.transform(stopwords_rem)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\t\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(3).fit(cv)\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    topic_list.append(\" \".join([terms[i] for i in topic.argsort()[:-6-1:-1]]))\n",
    "    print(topic_list[-1])\n",
    "lda.transform(cv[1])\n",
    "#print(lda.components_)\n",
    "\n",
    "df3['Topic'] = topic_dist.argmax(axis=1)\n",
    "\n",
    "df3[df3['Topic']==1]['description'].head()\n",
    "\n",
    "df3[df3['Topic']==0]['description'].head()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([terms[i] for i in topic.argsort()[:-15-1:-1]]))\n",
    "\n",
    "spam = df3[df3['Spam']==0]\n",
    "\n",
    "list1 = list(spam['Stop_new'])\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5)\n",
    "\n",
    "vectorizer.fit(list1)\n",
    "v3 = vectorizer.transform(list1)\n",
    "\n",
    "\n",
    "Feautur_name = vectorizer.get_feature_names()\n",
    "dense = v3.todense()\n",
    "dense_list = dense.tolist()\n",
    "\n",
    "X = array(dense_list)\n",
    "y = spam['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(5,activation='relu'))\n",
    "model.add(Dense(5,activation='relu'))\n",
    "model.add(Dense(3,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "model.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=500)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
